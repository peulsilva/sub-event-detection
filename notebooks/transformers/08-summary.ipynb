{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from src.utils import train_test_split, get_sample_weights, get_eval_set\n",
    "from src.preprocessing import preprocess_data\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from src.preprocessing import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, LoggingHandler\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "import re\n",
    "from src.utils import aggregate_samples, evaluate_model, compute_class_weights, remove_hashtag_links, get_first_texts\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import ast\n",
    "\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e9d326445f4a49b4cd914d5ce2691a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34569599ff94286b1060958fe0e8f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342e4b3989de47aeaa27cbd9b06d049d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9025dd2a604e25bc604f6574af9937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/pedro.silva/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 1472980/1472980 [01:24<00:00, 17360.61it/s]\n",
      "100%|██████████| 1472980/1472980 [00:01<00:00, 1001996.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(train_data)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\", )\n",
    "df['tokens'] = df['Tweet'].progress_apply(tokenizer.tokenize)\n",
    "\n",
    "target_words = [\n",
    "    \"goal\", \"penalty\", \"halftime\", \"full-time\", \"yellow\", \"red\",\n",
    "    \"kickoff\", \"extra time\", \"stoppage time\", \"foul\", \"offside\", \"handball\",\n",
    "    \"save\", \"tackle\", \"dribble\", \"corner\", \"substitution\", \"header\",\n",
    "    \"free kick\", \"throw-in\", \"assist\", \"hat-trick\", \"own goal\", \"victory\",\n",
    "    \"defeat\", \"draw\", \"win\", \"loss\", \"tie\", \"comeback\", \"goalkeeper\",\n",
    "    \"striker\", \"midfielder\", \"defender\", \"referee\", \"fans\", \"var\", \"gooal\"\n",
    "]\n",
    "target_words = set(tokenizer.tokenize(\" \".join(target_words)))\n",
    "\n",
    "def is_valid_text(t):\n",
    "    for w in t:\n",
    "        if w in target_words:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "df['is_valid']= df['tokens'].progress_apply(is_valid_text)\n",
    "# df['lan'] = df['Tweet'].progress_apply(lambda x : langid.classify(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df.query(\"is_valid == 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ts = valid_df.groupby([\"MatchID\", \"PeriodID\"]).Timestamp.min()\n",
    "last_ts = valid_df.groupby([\"MatchID\", \"PeriodID\"]).Timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/pedro.silva/Desktop/sub-event-detection/src/utils.py:157: FutureWarning: The provided callable <function mean at 0x7feaa0126020> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  return (df.query(f\"MatchID in {indices}\")).groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
      "/users/eleves-a/2022/pedro.silva/Desktop/sub-event-detection/src/utils.py:157: FutureWarning: The provided callable <function mean at 0x7feaa0126020> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  return (df.query(f\"MatchID in {indices}\")).groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
      "/users/eleves-a/2022/pedro.silva/Desktop/sub-event-detection/src/utils.py:157: FutureWarning: The provided callable <function mean at 0x7feaa0126020> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  return (df.query(f\"MatchID in {indices}\")).groupby([\"MatchID\", \"PeriodID\"]).agg({\n"
     ]
    }
   ],
   "source": [
    "possible_indices = set(train_data.keys())\n",
    "\n",
    "test_indices = list(np.random.choice(list(possible_indices), size=3, replace = False,))\n",
    "test_indices = [13,1,18]\n",
    "all_train_indices = list(possible_indices.difference(set(test_indices)))\n",
    "val_indices = [1,5,12,19]\n",
    "# val_indices = list(np.random.choice(all_train_indices, 3, replace=False))\n",
    "# train_indices = list(set(all_train_indices).difference(set(val_indices)))\n",
    "# train_indices = [0,2,7,11,13,18]\n",
    "\n",
    "\n",
    "\n",
    "train_df = aggregate_samples(valid_df, list(possible_indices), max_tweet_size = 10)\n",
    "test_df = aggregate_samples(valid_df, test_indices, max_tweet_size = 10)\n",
    "val_df = aggregate_samples(valid_df, val_indices, max_tweet_size=10)\n",
    "\n",
    "train_df = remove_hashtag_links(train_df)\n",
    "test_df = remove_hashtag_links(test_df)\n",
    "val_df = remove_hashtag_links(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = ''' \n",
    "\n",
    "\n",
    "You are a helpful AI tasked with analyzing a collection of tweets posted during a single minute of a football match. Your goal is to generate a concise summary of the key events that occurred during this time and to specifically answer whether any of the following events occurred: \n",
    "\n",
    "1. A goal (including who scored, if mentioned).\n",
    "2. A yellow or red card (including the player or team, if mentioned).\n",
    "3. A kickoff (start of a half or after a goal).\n",
    "4. Halftime or fulltime whistle.\n",
    "\n",
    "Here are the tweets:\n",
    "\n",
    "{tweets}\n",
    "\n",
    "### Instructions:\n",
    "1. Analyze the tweets for clear indications of the above events using common football-related terminology, phrases, or hashtags.\n",
    "2. If the event is ambiguous or not explicitly stated in the tweets, mark it as \"Not mentioned.\"\n",
    "3. Summarize any additional significant match events or fan reactions from the tweets that are relevant to understanding the minute.\n",
    "\n",
    "Return a detailed summary of the events, and mention if there were any cards, goals halftime or kickoff in that match. Be as precise as possible and say the name of the player if possible\n",
    "Try to reason as much as possible and avoid simple and quick answers\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"E816LUcagzqnrKK49x99jFqzVlrEDKqr\",\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    Generation of text index = 2136\n",
      "                    Generated text = After analyzing the tweets, I've identified the following key events that occurred during the single minute of the football match:\n",
      "\n",
      "1. **Goal**: Yes, multiple goals were mentioned in the tweets. The scores mentioned are:\n",
      "\t* 3-1 (twice)\n",
      "\t* 1-3\n",
      "This suggests that the team that was trailing 1-3 eventually scored a goal to bring the score to 3-1, but the original score of 3-1 is unclear.\n",
      "2. **Yellow or Red Card**: Not mentioned.\n",
      "3. **Kickoff**: Not mentioned. The tweets seem to be referring to a continuation of the match rather than the start of a new half.\n",
      "4. **Halftime or Fulltime Whistle**: Not mentioned.\n",
      "\n",
      "Additionally, significant fan reactions and match events that can be inferred from the tweets are:\n",
      "\n",
      "* The team scored multiple goals, with the final score being 3-1 (at least initially).\n",
      "* The team's goalkeeper, Ochoa, was commended for their performance ( twitter post \"Ochoa lost his virginity today 3-1\").\n",
      "* The team's fans were ecstatic, with several tweets expressing joy and excitement (e.g., \"I'm soo happy right now\", \"Let the drinking begin to that  win\", etc.).\n",
      "* The stadium was filled with energy, with fans singing a traditional Mexican song, \"Cielito Lindo\" (as mentioned in the tweet \"I love hearing the stadium singing Cielito Lindo\").\n",
      "\n",
      "It is worth noting that the tweets do not provide enough information to determine the names of the teams playing, but based on the hashtags and tweets, it appears that one of the teams is Mexico, and the other team is the Netherlands.\n",
      "                    Number of prompt tokens = 425\n",
      "                    Number of output tokens = 353\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"E816LUcagzqnrKK49x99jFqzVlrEDKqr\",\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "async def create_completion(prompt, model_name, stream):\n",
    "    try:\n",
    "        chat_completion = await asyncio.to_thread(\n",
    "            client.chat.completions.create,\n",
    "            model=model_name,\n",
    "            # prompt = prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            for event in chat_completion:\n",
    "                if event.choices[0].finish_reason:\n",
    "                    print(\n",
    "                        event.choices[0].finish_reason,\n",
    "                        event.usage['prompt_tokens'],\n",
    "                        event.usage['completion_tokens']\n",
    "                    )\n",
    "                else:\n",
    "                    print(event.choices[0].delta.content)\n",
    "            return None  # Return None for stream as the output is handled by the print statements\n",
    "        else:\n",
    "            generated_text = chat_completion.choices[0].message.content\n",
    "            prompt_tokens = chat_completion.usage.prompt_tokens\n",
    "            output_tokens = chat_completion.usage.completion_tokens\n",
    "            return {\n",
    "                \"generated_text\": generated_text,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"output_tokens\": output_tokens\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API request: {e}\")\n",
    "        return None\n",
    "\n",
    "async def process_completions(study_df, model_name, stream, batch_size=1_000):\n",
    "    generated_texts = []\n",
    "    \n",
    "    tasks = []\n",
    "    for i, (text_idx, row) in enumerate(study_df.iterrows()):\n",
    "        prompt = base_prompt.format(tweets=row['Tweet'])\n",
    "        tasks.append((i, text_idx, row, create_completion(prompt, model_name, stream)))\n",
    "\n",
    "    for batch_start in range(0, len(tasks), batch_size):\n",
    "        batch = tasks[batch_start:batch_start + batch_size]\n",
    "        results = await asyncio.gather(*[task[3] for task in batch])\n",
    "        \n",
    "        for (i, text_idx, row, _), result in zip(batch, results):\n",
    "            if result is not None:\n",
    "                generated_text = result['generated_text']\n",
    "                prompt_tokens = result['prompt_tokens']\n",
    "                output_tokens = result['output_tokens']\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                print(\n",
    "                    f\"\"\"\n",
    "                    Generation of text index = {i}\n",
    "                    Generated text = {generated_text}\n",
    "                    Number of prompt tokens = {prompt_tokens}\n",
    "                    Number of output tokens = {output_tokens}\n",
    "                    \"\"\"\n",
    "                )\n",
    "\n",
    "                generated_texts.append({\n",
    "                    \"generated_text\": generated_text,\n",
    "                    \"text\": row['Tweet'],\n",
    "                    \"text_idx\": text_idx,\n",
    "                    \"label\": row[\"EventType\"]\n",
    "                })\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "async def main():\n",
    "    # Define your study_df, model_name, stream, and other necessary variables\n",
    "    stream = False  # Change to True if you want streaming\n",
    "\n",
    "    # Run the process_completions coroutine\n",
    "    generated_texts = await process_completions(train_df, model_name, stream, batch_size=1000)\n",
    "    return generated_texts\n",
    "\n",
    "# Run the async main function\n",
    "generated_texts = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(generated_texts).to_pickle(\"summary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts['preds'] = generated_texts['generated_text'].str.lower().str.contains(\"yes\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts['text_idx'] = generated_texts['text_idx'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts['match_id'] = generated_texts['text_idx'].apply(lambda x: x[0])\n",
    "generated_texts['period_id'] = generated_texts['text_idx'].apply(lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts.groupby(\"match_id\").apply(lambda x: accuracy_score(x['label'], x['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_texts.query(\"match_id == 1\")['generated_text'].iloc[69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts.set_index(\"period_id\").query(\"match_id == 1\")[['label', 'preds']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = get_eval_set()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "eval_df['tokens'] = eval_df['Tweet'].progress_apply(tokenizer.tokenize)\n",
    "\n",
    "target_words = [\n",
    "    \"goal\", \"penalty\", \"halftime\", \"full-time\", \"yellow\", \"red\",\n",
    "    \"kickoff\", \"extra time\", \"stoppage time\", \"foul\", \"offside\", \"handball\",\n",
    "    \"save\", \"tackle\", \"dribble\", \"corner\", \"substitution\", \"header\",\n",
    "    \"free kick\", \"throw-in\", \"assist\", \"hat-trick\", \"own goal\", \"victory\",\n",
    "    \"defeat\", \"draw\", \"win\", \"loss\", \"tie\", \"comeback\", \"goalkeeper\",\n",
    "    \"striker\", \"midfielder\", \"defender\", \"referee\", \"fans\", \"var\", \"gooal\"\n",
    "]\n",
    "target_words = set(tokenizer.tokenize(\" \".join(target_words)))\n",
    "\n",
    "def is_valid_text(t):\n",
    "    for w in t:\n",
    "        if w in target_words:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "eval_df['is_valid']= eval_df['tokens'].progress_apply(is_valid_text)\n",
    "# df['lan'] = df['Tweet'].progress_apply(lambda x : langid.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['EventType'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_eval_df = aggregate_samples(eval_df.query(\"is_valid == 1\"), eval_df.MatchID.unique().tolist(), max_size=10)\n",
    "\n",
    "preprocessed_eval_df = remove_hashtag_links(preprocessed_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Define your study_df, model_name, stream, and other necessary variables\n",
    "    stream = False  # Change to True if you want streaming\n",
    "\n",
    "    # Run the process_completions coroutine\n",
    "    generated_texts = await process_completions(preprocessed_eval_df, model_name, stream, batch_size=1000)\n",
    "    return generated_texts\n",
    "\n",
    "# Run the async main function\n",
    "generated_texts = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated = pd.DataFrame(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(eval_df_generated))\n",
    "print(eval_df_generated['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_df_generated['generated_text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated['EventType'] = eval_df_generated['generated_text']\\\n",
    "    .str\\\n",
    "    .lower()\\\n",
    "    .str\\\n",
    "    .contains(\"yes\")\\\n",
    "    .astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated['MatchID'] = eval_df_generated['text_idx'].apply(lambda x : x[0])\n",
    "eval_df_generated['PeriodID'] = eval_df_generated['text_idx'].apply(lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = eval_df.drop_duplicates(\"ID\")[['ID', \"MatchID\", \"PeriodID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    eval_df_generated,\n",
    "    x,\n",
    "    on = ['MatchID', \"PeriodID\"]\n",
    "    \n",
    ")[[\"ID\", \"EventType\"]]\\\n",
    "    .set_index(\"ID\")\\\n",
    "    .to_csv(\"predictions_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated['EventType'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"generated_summary_llm_2.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['text_idx']= df['text_idx'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'] = df['generated_text'].str.lower().str.contains(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60)\n",
      "After analyzing the tweets, I have the following summary:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"summary\": \"The minute saw a series of intense matches between Germany and Argentina, with both teams showing significant aggression and skill. Argentina's goalkeeper was praised for their good game, while Germany's fans expressed frustration with their team's performance. At the end of the minute, the score was still 0-0, but fans were already anticipating a goal.\",\n",
      "    \"goal\": \"no\",\n",
      "    \"cards\": \"yes\",\n",
      "    \"kickoff\": \"no\",\n",
      "    \"halftime\": \"yes\",\n",
      "    \"fulltime\": \"no\"\n",
      "}\n",
      "```\n",
      "\n",
      "Here's the explanation for each event:\n",
      "\n",
      "1. **Goal**: There is no clear indication of a goal scored in that exact minute. Some tweets mention that Argentina \"nicked it\" or that Germany \"was so close\", but these are speculative and do not explicitly mention a goal. [Tweet that refers to the goal: None]\n",
      "\n",
      "2. **Cards**: There is a mention of \"foul trouble\" for Germany, which implies that there were some yellow cards given out, but this does not explicitly state that there were any cards in that exact minute. However, the presence of this tweet suggests that there might have been some carding events in the match. [Tweet that refers to the card: \"Foul trouble for Germany\"]\n",
      "\n",
      "3. **Kickoff**: There is no indication that the match just started in that exact minute. The tweets suggest that the game has been ongoing for some time, with the teams already showing signs of fatigue and frustration. [Tweet that refers to the kickoff: None]\n",
      "\n",
      "4. **Halftime**: The tweet \"Half-time Argentina 0-0 Germany\" explicitly states that the halftime whistle has been blown, ending the first half. [Tweet that refers to the halftime: \"Half-time Argentina 0-0 Germany\"]\n",
      "\n",
      "5. **Fulltime**: There is no indication that the game has ended in that exact minute. The tweets suggest that the first half has ended, but the game is still ongoing. [Tweet that refers to the fulltime: None]\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(df.query(\"label == 0 and prediction == 1\")))\n",
    "print(df.query(\"label == 0 and prediction == 1\")['text_idx'].iloc[idx])\n",
    "print(df.query(\"label == 0 and prediction == 1\")['generated_text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 13)\n",
      "Welcome back Pepe! \n",
      "Also   \n",
      "Let's go  !!!!\n",
      "good luck for \n",
      "Let's go  !\n",
      "  -  \n",
      "LET'S GO  AND !!!\n",
      "Switching over to  \n",
      " &  lets goooooooo!\n",
      "do not disappoint me \n",
      "Kick off :  v \n",
      " &  all fucking day\n",
      "I'm With   & \n",
      "Do or die!   \n",
      "I'm prediction -2 -1\n",
      " and -3 -1\n",
      "Its do or die lets go! \n",
      " vs  \n",
      " vs  \n",
      "nice and sunny for  - \n",
      "ima need  to beat  today!!\n",
      "One  goal and we can relax\n",
      "THE BAE HES SO BEAUTIFYL  \n",
      "Games underway\n",
      " vs \n",
      " vs   \n",
      "\"0\" kick off  0-0  Live TVONE\"\n",
      "If only  had a World Class Striker..!!\n",
      "For   back in the first 11.\n",
      "\n",
      "Come on  !! Min 2-0!! Let's goo!!\n",
      "lmao no one on the left side \n",
      "eder is awful why is he there \n",
      "Watching  V  ..Hoping for nothing :(\n",
      "We can do this, I know we can \n",
      "Which channel is the  vs  match on?\n",
      "\n",
      "I really want  n  to win today\n",
      "hope  qualified for the next round ! \n",
      "I may be the only person watching  \n",
      "hoy voy con    y  \n",
      "Game on ! Btw   |  \n",
      "Let's go  we need the win, 3-0 \n",
      "I hope  wins and I hope  wins\n"
     ]
    }
   ],
   "source": [
    "print(df.query(\"label == 0 and prediction == 1\")['text_idx'].iloc[idx])\n",
    "print(df.query(\"label == 0 and prediction == 1\")['text'].iloc[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
