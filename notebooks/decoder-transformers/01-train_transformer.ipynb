{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from src.utils import train_test_split, get_sample_weights, get_eval_set\n",
    "from src.preprocessing import preprocess_data\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from src.preprocessing import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, LoggingHandler\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import re\n",
    "from bert_score import BERTScorer\n",
    "import langid\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/pedro.silva/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scorer = BERTScorer(model_type='bert-base-uncased',num_layers = 12, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_texts(x, max_size = 10):\n",
    "    size = x.apply(lambda x: len(x.split(\" \")))\\\n",
    "        .sort_values()\n",
    "    \n",
    "    x = x.reindex_like(size)\n",
    "    mask = size < max_size\n",
    "    # mask = x.str.lower().str.contains(\"goal\")\n",
    "\n",
    "    return \"\\n\".join(x[mask])\n",
    "    # return x[mask].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split()\n",
    "def get_samples(indices, frac = 1, df = None):\n",
    "    all_df = []\n",
    "\n",
    "    if df is None:\n",
    "        for id in indices:\n",
    "            temp_df = train_data[id]\n",
    "\n",
    "            \n",
    "            \n",
    "            all_df.append(temp_df.dropna().sample(frac=frac))\n",
    "\n",
    "            \n",
    "        return pd.concat(all_df).groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
    "            \"Tweet\":    get_first_texts,\n",
    "            \"EventType\": np.mean,\n",
    "            \"ID\": len\n",
    "        })\n",
    "    \n",
    "    else: \n",
    "        return (df.query(f\"MatchID in {indices}\")).groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
    "            \"Tweet\":    get_first_texts,\n",
    "            \"EventType\": np.mean,\n",
    "            \"ID\": len\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = \"Threre was a goal, half time, kick-off, full time, penalty, red card, yellow card, or own goal\"\n",
    "# # Define batch size\n",
    "# batch_size = 1024\n",
    "\n",
    "# # Initialize lists to store scores\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "\n",
    "# data = pd.concat(train_data.values())\n",
    "\n",
    "# # Create a progress bar\n",
    "# for i in tqdm(range(0, len(data), batch_size), desc=\"Scoring Batches\"):\n",
    "#     # Slice the batch\n",
    "#     batch = data.iloc[i:i + batch_size]['Tweet'].tolist()\n",
    "    \n",
    "#     # Compute BERTScore for the batch\n",
    "#     P, R, F1 = scorer.score(batch, [sample_text] * len(batch), )\n",
    "    \n",
    "#     # Append scores\n",
    "#     precisions.extend(P.tolist())\n",
    "#     recalls.extend(R.tolist())\n",
    "#     f1_scores.extend(F1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.concat(train_data.values())\n",
    "# all_df['bertscore'] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = all_df.groupby([\"MatchID\", \"PeriodID\"], as_index=False).apply(lambda x: x.sort_values(\"bertscore\").iloc[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithExtraFeature(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, extra_feature_size=1):\n",
    "        super(BertWithExtraFeature, self).__init__()\n",
    "        # Load the pre-trained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name, cache_dir = '/Data')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.extra_feature_size = extra_feature_size\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        # Fully connected layer to combine BERT output and extra feature\n",
    "        self.fc = torch.nn.Linear(self.hidden_size + self.extra_feature_size, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, extra_feature):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (batch_size, seq_len) with token IDs.\n",
    "            attention_mask: Tensor of shape (batch_size, seq_len) for masking attention.\n",
    "            extra_feature: Tensor of shape (batch_size, 1) with the additional feature.\n",
    "\n",
    "        Returns:\n",
    "            Logits for binary classification.\n",
    "        \"\"\"\n",
    "        # Get BERT output (pooled output)\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.pooler_output  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Concatenate the pooled output with the extra feature\n",
    "        combined_input = torch.cat((pooled_output, extra_feature), dim=1)  # Shape: (batch_size, hidden_size + extra_feature_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        logits = self.fc(combined_input)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "def evaluate_model(\n",
    "    val_df: pd.DataFrame, \n",
    "    val_dataloader, \n",
    "    model, device : str = 'cuda', \n",
    "    use_labels = True, \n",
    "    sample_weight = None,\n",
    "    extra_feature : bool = False,\n",
    "    return_proba  =False\n",
    "):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    predict_proba = []\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type = 'cuda'):\n",
    "            for i,batch in tqdm(enumerate(val_dataloader), total = len(val_dataloader)):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                count = batch['count'].to(device).unsqueeze(dim = -1)\n",
    "\n",
    "                labels = None\n",
    "                if  use_labels:\n",
    "                    labels = batch[\"label\"].to(device)\n",
    "                # count = batch['count'].to(device).unsqueeze(dim = -1)\n",
    "\n",
    "                if extra_feature:\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, extra_feature = count)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                else:\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    preds = torch.argmax(outputs.logits, dim=1)\n",
    "                    probas = torch.softmax(outputs.logits, dim = 1)[:,1]\n",
    "                    \n",
    "                predict_proba.extend(probas.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                if use_labels:\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                # if i % 100 == 0: \n",
    "                #     acc = accuracy_score(all_labels, all_preds)\n",
    "                #     f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "                #     clear_output()\n",
    "                #     print(f\"Validation Accuracy : {acc}\\n\")\n",
    "                #     print(f\"Validation F1 : {f1}\\n\")\n",
    "                #     conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "                #     print(conf_matrix)\n",
    "\n",
    "    if use_labels:\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Validation Accuracy : {acc}\\n\")\n",
    "        print(f\"Validation auc : {auc}\\n\")\n",
    "        print(conf_matrix)\n",
    "\n",
    "    if return_proba:\n",
    "        return all_preds, all_labels, predict_proba\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "    return torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtag_links(df):\n",
    "\n",
    "    df['Tweet'] = df['Tweet'].str.replace(r\"#\\w+\", \"\", regex=True)\n",
    "\n",
    "    # Remove links\n",
    "    df['Tweet'] = df['Tweet'].str.replace(r\"http\\S+|www\\S+\", \"\", regex=True)\n",
    "\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and pictographs extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    df['Tweet'] = df['Tweet'].str.replace(emoji_pattern, \"\", regex=True)\n",
    "    df['Tweet'] = df['Tweet'].str.strip()\n",
    "\n",
    "    # df['Tweet'] = \"Is there any event like goal, halftime, fulltime, start of match or cards in any of the following tweets?\\n\\n\" + df['Tweet']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(train_data)\n",
    "df = remove_hashtag_links(df)\n",
    "# df['lan'] = df['Tweet'].progress_apply(lambda x : langid.classify(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = df#.query(\"lan == 'en' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_indices = set(train_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_528639/1023686446.py:14: FutureWarning: The provided callable <function mean at 0x7f4c28395d00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  return pd.concat(all_df).groupby([\"MatchID\", \"PeriodID\"]).agg({\n"
     ]
    }
   ],
   "source": [
    "test_indices = list(np.random.choice(list(possible_indices), size=3, replace = False,))\n",
    "test_indices = [13,1,18]\n",
    "all_train_indices = list(possible_indices.difference(set(test_indices)))\n",
    "val_indices = [1,5,12,19]\n",
    "# val_indices = list(np.random.choice(all_train_indices, 3, replace=False))\n",
    "# train_indices = list(set(all_train_indices).difference(set(val_indices)))\n",
    "train_indices = [0,2,7,11,13,18]\n",
    "\n",
    "\n",
    "train_df = get_samples(train_indices,)\n",
    "# train_df = get_samples(train_indices, df = train)\n",
    "test_df = get_samples(test_indices)\n",
    "val_df = get_samples(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = remove_hashtag_links(train_df)\n",
    "test_df = remove_hashtag_links(test_df)\n",
    "val_df = remove_hashtag_links(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 14, 1]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.6931818181818182\n",
      "\n",
      "Validation auc : 0.6933850129198966\n",
      "\n",
      "[[154  71]\n",
      " [ 64 151]]\n"
     ]
    }
   ],
   "source": [
    "# K Fold CV\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", cache_dir = '/Data')\n",
    "\n",
    "device = 'cuda'\n",
    "final_results = []\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    train_df[\"Tweet\"].tolist(), \n",
    "    train_df['ID'].tolist(),\n",
    "    train_df[\"EventType\"].tolist(), \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = TextDataset(\n",
    "    val_df[\"Tweet\"].tolist(), \n",
    "    val_df['ID'].tolist(),\n",
    "    val_df[\"EventType\"].tolist(), \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    test_df[\"Tweet\"].tolist(), \n",
    "    test_df['ID'].tolist(),\n",
    "    test_df[\"EventType\"].tolist(), \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", cache_dir = '/Data', num_labels = 2, dropout = 0.4)\n",
    "# model.resize_position_embeddings(2048)\n",
    "\n",
    "# model = BertWithExtraFeature(bert_model_name=\"bert-base-uncased\")\n",
    "\n",
    "# for p in model.distlbert.parameters():\n",
    "#     p.requires_grad = False\n",
    "model.to(device)\n",
    "\n",
    "# model = get_peft_model(base_model, lora_config)\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4,)\n",
    "\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params': model.bert.parameters(), 'weight_decay': 1e-3},  # Regularize BERT weights\n",
    "#     {'params': model.fc.parameters(), 'weight_decay': 1e-2}     # Stronger regularization on the classifier\n",
    "# ], lr=1e-5)\n",
    "\n",
    "for name, param in model.distilbert.named_parameters():\n",
    "    if \"layer.5\" in name or \"layer.4\" in name:  # Unfreeze last two layers\n",
    "        param.requires_grad = True\n",
    "\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "best_model = None\n",
    "second_best_model = None\n",
    "best_acc = -1\n",
    "second_best_acc = -1\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "labels = train_df[\"EventType\"].tolist()\n",
    "class_weight = torch.Tensor([0.4, 0.6]).to(device)\n",
    "# class_weights = compute_class_weights(labels).to(device)\n",
    "\n",
    "# Define weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(class_weight)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    epoch_loss = 0\n",
    "\n",
    "    print(train_indices, val_indices)\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        count = batch['count'].to(device).unsqueeze(dim = -1)\n",
    "\n",
    "        with torch.autocast( device_type = 'cuda'):\n",
    "            # outputs = model(input_ids=input_ids, attention_mask=attention_mask, extra_feature = count)\n",
    "            # loss = loss_fn(outputs, labels.squeeze() )\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels = labels)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            # loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        # preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    acc_train = accuracy_score(all_labels, all_preds, )\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"---------- Epoch {epoch} ------------\")\n",
    "    print(f\"Training Loss : {epoch_loss}\\n\")\n",
    "    print(f\"Training Accuracy : {acc_train}\\n\")\n",
    "    print(f\"Training Precision : {precision}\\n\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "    preds, labels = evaluate_model(val_df, val_dataloader, model, extra_feature=False)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # if acc_train > 0.8 and acc > 0.68:\n",
    "    #     best_model = deepcopy(model)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        second_best_acc = best_acc\n",
    "        second_best_model = deepcopy(best_model)  # Promote previous best to second best\n",
    "        best_acc = acc\n",
    "        best_model = deepcopy(model)\n",
    "    # elif acc > second_best_acc:\n",
    "    #     second_best_acc = acc\n",
    "    #     second_best_model = deepcopy(model)\n",
    "\n",
    "\n",
    "# Combine results for this fold\n",
    "# validation_results = pd.DataFrame({\n",
    "#     \"MatchID\": validation_data[\"MatchID\"].values,\n",
    "#     \"true_values\": labels,\n",
    "#     \"predictions\": preds,\n",
    "# })\n",
    "\n",
    "# final_results.append(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# class TweetDataset(Dataset):\n",
    "#     def __init__(self, tweets, tokenizer, max_length=512):\n",
    "#         self.tweets = tweets\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.tweets)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.tokenizer(\n",
    "#             self.tweets[idx],\n",
    "#             return_tensors='pt',\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             max_length=self.max_length\n",
    "#         )\n",
    "\n",
    "# # Define a dataset\n",
    "\n",
    "# def get_X(texts):\n",
    "#     tweet_dataset = TweetDataset(\n",
    "#         tweets=texts,\n",
    "#         tokenizer=tokenizer,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "#     # Create a DataLoader for batching\n",
    "#     batch_size = 32  # Adjust batch size based on available GPU memory\n",
    "#     data_loader = DataLoader(tweet_dataset, batch_size=batch_size)\n",
    "\n",
    "#     X_train_list = []  # To store processed embeddings\n",
    "\n",
    "#     # Process in batches\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(data_loader):\n",
    "#             # Move tokenized input to GPU\n",
    "#             b = {k: v.squeeze().to(\"cuda\") for k, v in batch.items()}  # Squeeze to match model input shape\n",
    "            \n",
    "#             # Model forward pass\n",
    "#             outputs = best_model.distilbert(**b)\n",
    "            \n",
    "#             # Collect the embeddings (modify based on your use case, e.g., logits, hidden states, etc.)\n",
    "#             X_train_list.append(outputs.last_hidden_state[:,-1,:].to(\"cpu\"))\n",
    "\n",
    "#     # Concatenate all batches if needed\n",
    "#     X = torch.cat(X_train_list, dim=0)\n",
    "    \n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:12<00:00,  3.22it/s]\n",
      "100%|██████████| 14/14 [00:04<00:00,  3.03it/s]\n",
      "100%|██████████| 13/13 [00:04<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = get_X(train_df['Tweet'].tolist())\n",
    "X_val = get_X(val_df['Tweet'].tolist())\n",
    "X_test = get_X(test_df['Tweet'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6295454545454545, 0.7128205128205128)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=8,max_features='log2', class_weight={0:0.4, 1:0.6})\n",
    "\n",
    "clf.fit(X_train, train_df['EventType'])\n",
    "\n",
    "y_pred_val = clf.predict(X_val)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(val_df['EventType'],y_pred_val), accuracy_score(test_df['EventType'],y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 85,  76],\n",
       "       [ 40, 189]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(val_df['EventType'],y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.6846153846153846\n",
      "\n",
      "Validation auc : 0.6778722030981066\n",
      "\n",
      "[[105  61]\n",
      " [ 62 162]]\n"
     ]
    }
   ],
   "source": [
    "preds, labels = evaluate_model(test_df, test_dataloader, best_model, extra_feature=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1471685/2907081575.py:21: FutureWarning: The provided callable <function mean at 0x7f1bec12dee0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  return (df.query(f\"MatchID in {indices}\")).groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
      "/tmp/ipykernel_1471685/2907081575.py:21: FutureWarning: The provided callable <function mean at 0x7f1bec12dee0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  return (df.query(f\"MatchID in {indices}\")).groupby([\"MatchID\", \"PeriodID\"]).agg({\n"
     ]
    }
   ],
   "source": [
    "# val_indices = list(np.random.choice(list(train_data.keys()), size=6, replace = False))\n",
    "# train_indices = list(set(train_data.keys()).difference(set(val_indices)))\n",
    "\n",
    "val_indices = [1,5,12,19]\n",
    "# val_indices = list(np.random.choice(all_train_indices, 3, replace=False))\n",
    "# train_indices = list(set(all_train_indices).difference(set(val_indices)))\n",
    "train_indices = [0,2,7,11,13,18]\n",
    "\n",
    "\n",
    "\n",
    "# train_df = get_samples(train_indices, df = train)\n",
    "train_df = get_samples(train_indices, df = en_df)\n",
    "val_df = get_samples(val_indices, df = en_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = remove_hashtag_links(train_df, )\n",
    "val_df = remove_hashtag_links(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventType\n",
       "1.0    0.567949\n",
       "0.0    0.432051\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['EventType'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventType\n",
       "1.0    0.558522\n",
       "0.0    0.441478\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['EventType'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8700185538924b1a91a2844a775198a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d269af8fdcea4561abccf0f35928bc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", cache_dir = '/Data', num_labels = 2,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.675564681724846\n",
      "\n",
      "Validation auc : 0.6652103283173735\n",
      "\n",
      "[[124  91]\n",
      " [ 67 205]]\n"
     ]
    }
   ],
   "source": [
    "# K Fold CV\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\", cache_dir = '/Data')\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "final_results = []\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    train_df[\"Tweet\"].tolist(), \n",
    "    train_df['ID'].tolist(),\n",
    "    train_df[\"EventType\"].tolist(), \n",
    "    tokenizer,\n",
    "    train_df.index.get_level_values(\"MatchID\").tolist()\n",
    "\n",
    ")\n",
    "\n",
    "val_dataset = TextDataset(\n",
    "    val_df[\"Tweet\"].tolist(), \n",
    "    val_df['ID'].tolist(),\n",
    "    val_df[\"EventType\"].tolist(), \n",
    "    tokenizer,\n",
    "    val_df.index.get_level_values(\"MatchID\").tolist()\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", cache_dir = '/Data', num_labels = 2,ignore_mismatched_sizes=True)\n",
    "# for p in base_model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "model.to(device)\n",
    "\n",
    "# model = get_peft_model(base_model, lora_config)\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "for name, param in model.longformer.named_parameters():\n",
    "    if \"layer.11\" in name:  # Unfreeze last two layers\n",
    "        param.requires_grad = True\n",
    "\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "labels = train_df[\"EventType\"].tolist()\n",
    "class_weights = torch.Tensor([0.4,0.6]).to(device)\n",
    "\n",
    "# Define weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    epoch_loss = 0\n",
    "\n",
    "    print(train_indices, val_indices)\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.autocast( device_type = 'cuda'):\n",
    "            # outputs = model(input_ids=input_ids, attention_mask=attention_mask, extra_feature = count)\n",
    "            # loss = loss_fn(outputs, labels.squeeze() )\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels = labels)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds, )\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"---------- Epoch {epoch} ------------\")\n",
    "    print(f\"Training Loss : {epoch_loss}\\n\")\n",
    "    print(f\"Training Accuracy : {acc}\\n\")\n",
    "    print(f\"Training Precision : {precision}\\n\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "    preds, labels = evaluate_model(val_df, val_dataloader, model)\n",
    "\n",
    "    acc = roc_auc_score(labels, preds)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = deepcopy(model)\n",
    "\n",
    "\n",
    "# Combine results for this fold\n",
    "# validation_results = pd.DataFrame({\n",
    "#     \"MatchID\": validation_data[\"MatchID\"].values,\n",
    "#     \"true_values\": labels,\n",
    "#     \"predictions\": preds,\n",
    "# })\n",
    "\n",
    "# final_results.append(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "total_test_df = get_eval_set().set_index([\"MatchID\", \"PeriodID\"])\n",
    "test_df = preprocess_data(total_test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = \"Threre was a goal, half time, kick-off, full time, penalty, red card, yellow card, or own goal\"\n",
    "# # Define batch size\n",
    "# batch_size = 1024\n",
    "\n",
    "# # Initialize lists to store scores\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# # f1_scores = []\n",
    "\n",
    "# data = test_df\n",
    "\n",
    "# # Create a progress bar\n",
    "# for i in tqdm(range(0, len(data), batch_size), desc=\"Scoring Batches\"):\n",
    "#     # Slice the batch\n",
    "#     batch = data.iloc[i:i + batch_size]['Tweet'].tolist()\n",
    "    \n",
    "#     # Compute BERTScore for the batch\n",
    "#     P, R, F1 = scorer.score(batch, [sample_text] * len(batch), )\n",
    "    \n",
    "#     # Append scores\n",
    "#     precisions.extend(P.tolist())\n",
    "#     recalls.extend(R.tolist())\n",
    "    # f1_scores.extend(F1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['bertscore'] = f1_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = test_df.groupby([\"MatchID\", \"PeriodID\"], as_index=False).apply(lambda x: x.sort_values(\"bertscore\").iloc[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362397/362397 [04:03<00:00, 1490.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_df['lan'] = test_df['Tweet'].progress_apply(langid.classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['language'] = test_df['lan'].apply(lambda x: x[0])\n",
    "test_df['language'] = \"en\"\n",
    "\n",
    "test_df_en = test_df.query(\"language == 'en' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_df = test_df_en.groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
    "    \"Tweet\":    get_first_texts,\n",
    "    \"ID\": len\n",
    "})\n",
    "\n",
    "processed_test_df = remove_hashtag_links(processed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">6</th>\n",
       "      <th>0</th>\n",
       "      <td>I Finally get to see Germany play\\n   \\nFascin...</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"In a few minutes  of  x ...Can't wait\"....Waa...</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't see any team in this World Cup that ca...</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Future World Cup champions are about to play.....</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thing is.. Ozil looks like one of my aquarium ...</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">16</th>\n",
       "      <th>125</th>\n",
       "      <td>Someone here just described Group D as \"litera...</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Stat comparison between Serbia &amp; Germany s int...</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>OMFG  just lost??? I'm glad I'm at work then! ...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ç›´å‰äºˆæƒ³0-0ã¯å¤–ã‚Œã€‚ã‚»ãƒ«ãƒ“ã‚¢å„ªä½ã...</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>:)))) Nije mala, nije mala...   !!!!!!\\nFive h...</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>516 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Tweet   ID\n",
       "MatchID PeriodID                                                        \n",
       "6       0         I Finally get to see Germany play\\n   \\nFascin...  237\n",
       "        1         \"In a few minutes  of  x ...Can't wait\"....Waa...  245\n",
       "        2         I don't see any team in this World Cup that ca...  254\n",
       "        3         Future World Cup champions are about to play.....  344\n",
       "        4         Thing is.. Ozil looks like one of my aquarium ...  456\n",
       "...                                                             ...  ...\n",
       "16      125       Someone here just described Group D as \"litera...  344\n",
       "        126       Stat comparison between Serbia & Germany s int...  315\n",
       "        127       OMFG  just lost??? I'm glad I'm at work then! ...  300\n",
       "        128       ç›´å‰äºˆæƒ³0-0ã¯å¤–ã‚Œã€‚ã‚»ãƒ«ãƒ“ã‚¢å„ªä½ã...  273\n",
       "        129       :)))) Nije mala, nije mala...   !!!!!!\\nFive h...  241\n",
       "\n",
       "[516 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(\n",
    "    processed_test_df[\"Tweet\"].tolist(), \n",
    "    processed_test_df['ID'].tolist(), \n",
    "    None,\n",
    "    tokenizer,\n",
    "    [0] * 516\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:42<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "preds, labels, probas = evaluate_model(processed_test_df, test_dataloader, best_model, use_labels=False, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(probas) > 0.4).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_df['EventType'] = (np.array(probas) > 0.3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventType\n",
       "1    0.563953\n",
       "0    0.436047\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_df['EventType'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_submission = pd.read_csv(\"sub-event-detection-in-twitter-streams/challenge_data/logistic_predictions.csv\").set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pd.merge(\n",
    "    total_test_df,\n",
    "    processed_test_df[[\"Tweet\", \"EventType\"]],\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    "\n",
    ")[['EventType','ID']]\\\n",
    "    .drop_duplicates(\"ID\")\\\n",
    "    .set_index(\"ID\")\\\n",
    "    .reindex(index = example_submission.index)\\\n",
    "    .to_csv(\"predictions_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventType\n",
       "0    0.831609\n",
       "1    0.168391\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
