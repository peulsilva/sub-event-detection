{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from src.utils import train_test_split, get_sample_weights, get_eval_set\n",
    "from src.preprocessing import preprocess_data\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from src.preprocessing import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, LoggingHandler\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "import re\n",
    "from src.utils import aggregate_samples, evaluate_model, compute_class_weights, remove_hashtag_links, get_first_texts\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import ast\n",
    "\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(train_data)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\", )\n",
    "df['tokens'] = df['Tweet'].progress_apply(tokenizer.tokenize)\n",
    "\n",
    "target_words = [\n",
    "    \"goal\", \"penalty\", \"halftime\", \"full-time\", \"yellow\", \"red\",\n",
    "    \"kickoff\", \"extra time\", \"stoppage time\", \"foul\", \"offside\", \"handball\",\n",
    "    \"save\", \"tackle\", \"dribble\", \"corner\", \"substitution\", \"header\",\n",
    "    \"free kick\", \"throw-in\", \"assist\", \"hat-trick\", \"own goal\", \"victory\",\n",
    "    \"defeat\", \"draw\", \"win\", \"loss\", \"tie\", \"comeback\", \"goalkeeper\",\n",
    "    \"striker\", \"midfielder\", \"defender\", \"referee\", \"fans\", \"var\", \"gooal\"\n",
    "]\n",
    "target_words = set(tokenizer.tokenize(\" \".join(target_words)))\n",
    "\n",
    "def is_valid_text(t):\n",
    "    for w in t:\n",
    "        if w in target_words:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "df['is_valid']= df['tokens'].progress_apply(is_valid_text)\n",
    "# df['lan'] = df['Tweet'].progress_apply(lambda x : langid.classify(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df.query(\"is_valid == 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ts = valid_df.groupby([\"MatchID\", \"PeriodID\"]).Timestamp.min()\n",
    "last_ts = valid_df.groupby([\"MatchID\", \"PeriodID\"]).Timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_indices = set(train_data.keys())\n",
    "\n",
    "test_indices = list(np.random.choice(list(possible_indices), size=3, replace = False,))\n",
    "test_indices = [13,1,18]\n",
    "all_train_indices = list(possible_indices.difference(set(test_indices)))\n",
    "val_indices = [1,5,12,19]\n",
    "# val_indices = list(np.random.choice(all_train_indices, 3, replace=False))\n",
    "# train_indices = list(set(all_train_indices).difference(set(val_indices)))\n",
    "# train_indices = [0,2,7,11,13,18]\n",
    "\n",
    "\n",
    "\n",
    "train_df = aggregate_samples(valid_df, list(possible_indices), max_size = 10)\n",
    "test_df = aggregate_samples(valid_df, test_indices, max_size=10)\n",
    "val_df = aggregate_samples(valid_df, val_indices, max_size=10)\n",
    "\n",
    "train_df = remove_hashtag_links(train_df)\n",
    "test_df = remove_hashtag_links(test_df)\n",
    "val_df = remove_hashtag_links(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = ''' \n",
    "\n",
    "\n",
    "You are a helpful AI tasked with analyzing a collection of tweets posted during a single minute of a football match. Your goal is to generate a concise summary of the key events that occurred during this time and to specifically answer whether any of the following events occurred: \n",
    "\n",
    "1. A goal (including who scored, if mentioned).\n",
    "2. A yellow or red card (including the player or team, if mentioned).\n",
    "3. A kickoff (start of a half or after a goal).\n",
    "4. Halftime or fulltime whistle.\n",
    "\n",
    "Here are the tweets:\n",
    "\n",
    "{tweets}\n",
    "\n",
    "### Instructions:\n",
    "1. Analyze the tweets for clear indications of the above events using common football-related terminology, phrases, or hashtags.\n",
    "2. If the event is ambiguous or not explicitly stated in the tweets, mark it as \"Not mentioned.\"\n",
    "3. Summarize any additional significant match events or fan reactions from the tweets that are relevant to understanding the minute.\n",
    "4. Return the response in the following structured format:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"summary\": [What happened in that minute.],\n",
    "    \"goal\": [yes/no/not mentioned if there was a goal on that exact minute. \n",
    "             If there was only a chance/attempt, return \"no\". \n",
    "             Add the tweet that refers to the goal.],\n",
    "    \"cards\": [yes/no/not mentioned if there were any cards on that exact minute. \n",
    "              Add the tweet that refers to the card.],\n",
    "    \"kickoff\": [yes/no/not mentioned if the match just started in that exact minute. \n",
    "                Add the tweet that refers to the kickoff.],\n",
    "    \"halftime\": [yes/no/not mentioned if the first half ended in that exact minute. \n",
    "                 Add the tweet that refers to the halftime.],\n",
    "    \"fulltime\": [yes/no/not mentioned if the game ended in that exact minute. \n",
    "                 Add the tweet that refers to the fulltime.]\n",
    "}}\n",
    "```\n",
    "Try to reason as much as possible and avoid simple and quick answers\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"E816LUcagzqnrKK49x99jFqzVlrEDKqr\",\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"E816LUcagzqnrKK49x99jFqzVlrEDKqr\",\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "async def create_completion(prompt, model_name, stream):\n",
    "    try:\n",
    "        chat_completion = await asyncio.to_thread(\n",
    "            client.chat.completions.create,\n",
    "            model=model_name,\n",
    "            # prompt = prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            for event in chat_completion:\n",
    "                if event.choices[0].finish_reason:\n",
    "                    print(\n",
    "                        event.choices[0].finish_reason,\n",
    "                        event.usage['prompt_tokens'],\n",
    "                        event.usage['completion_tokens']\n",
    "                    )\n",
    "                else:\n",
    "                    print(event.choices[0].delta.content)\n",
    "            return None  # Return None for stream as the output is handled by the print statements\n",
    "        else:\n",
    "            generated_text = chat_completion.choices[0].message.content\n",
    "            prompt_tokens = chat_completion.usage.prompt_tokens\n",
    "            output_tokens = chat_completion.usage.completion_tokens\n",
    "            return {\n",
    "                \"generated_text\": generated_text,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"output_tokens\": output_tokens\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API request: {e}\")\n",
    "        return None\n",
    "\n",
    "async def process_completions(study_df, model_name, stream, batch_size=1_000):\n",
    "    generated_texts = []\n",
    "    \n",
    "    tasks = []\n",
    "    for i, (text_idx, row) in enumerate(study_df.iterrows()):\n",
    "        prompt = base_prompt.format(tweets=row['Tweet'])\n",
    "        tasks.append((i, text_idx, row, create_completion(prompt, model_name, stream)))\n",
    "\n",
    "    for batch_start in range(0, len(tasks), batch_size):\n",
    "        batch = tasks[batch_start:batch_start + batch_size]\n",
    "        results = await asyncio.gather(*[task[3] for task in batch])\n",
    "        \n",
    "        for (i, text_idx, row, _), result in zip(batch, results):\n",
    "            if result is not None:\n",
    "                generated_text = result['generated_text']\n",
    "                prompt_tokens = result['prompt_tokens']\n",
    "                output_tokens = result['output_tokens']\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                print(\n",
    "                    f\"\"\"\n",
    "                    Generation of text index = {i}\n",
    "                    Generated text = {generated_text}\n",
    "                    Number of prompt tokens = {prompt_tokens}\n",
    "                    Number of output tokens = {output_tokens}\n",
    "                    \"\"\"\n",
    "                )\n",
    "\n",
    "                generated_texts.append({\n",
    "                    \"generated_text\": generated_text,\n",
    "                    \"text\": row['Tweet'],\n",
    "                    \"text_idx\": text_idx,\n",
    "                    \"label\": row[\"EventType\"]\n",
    "                })\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "async def main():\n",
    "    # Define your study_df, model_name, stream, and other necessary variables\n",
    "    stream = False  # Change to True if you want streaming\n",
    "\n",
    "    # Run the process_completions coroutine\n",
    "    generated_texts = await process_completions(train_df, model_name, stream, batch_size=1)\n",
    "    return generated_texts\n",
    "\n",
    "# Run the async main function\n",
    "generated_texts = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = pd.read_csv(\"generated_summary_llm_2.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts['preds'] = generated_texts['generated_text'].str.lower().str.contains(\"yes\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts['text_idx'] = generated_texts['text_idx'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts['match_id'] = generated_texts['text_idx'].apply(lambda x: x[0])\n",
    "generated_texts['period_id'] = generated_texts['text_idx'].apply(lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts.groupby(\"match_id\").apply(lambda x: accuracy_score(x['label'], x['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_texts.query(\"match_id == 1\")['generated_text'].iloc[69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts.set_index(\"period_id\").query(\"match_id == 1\")[['label', 'preds']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = get_eval_set()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "eval_df['tokens'] = eval_df['Tweet'].progress_apply(tokenizer.tokenize)\n",
    "\n",
    "target_words = [\n",
    "    \"goal\", \"penalty\", \"halftime\", \"full-time\", \"yellow\", \"red\",\n",
    "    \"kickoff\", \"extra time\", \"stoppage time\", \"foul\", \"offside\", \"handball\",\n",
    "    \"save\", \"tackle\", \"dribble\", \"corner\", \"substitution\", \"header\",\n",
    "    \"free kick\", \"throw-in\", \"assist\", \"hat-trick\", \"own goal\", \"victory\",\n",
    "    \"defeat\", \"draw\", \"win\", \"loss\", \"tie\", \"comeback\", \"goalkeeper\",\n",
    "    \"striker\", \"midfielder\", \"defender\", \"referee\", \"fans\", \"var\", \"gooal\"\n",
    "]\n",
    "target_words = set(tokenizer.tokenize(\" \".join(target_words)))\n",
    "\n",
    "def is_valid_text(t):\n",
    "    for w in t:\n",
    "        if w in target_words:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "eval_df['is_valid']= eval_df['tokens'].progress_apply(is_valid_text)\n",
    "# df['lan'] = df['Tweet'].progress_apply(lambda x : langid.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['EventType'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_eval_df = aggregate_samples(eval_df.query(\"is_valid == 1\"), eval_df.MatchID.unique().tolist(), max_size=10)\n",
    "\n",
    "preprocessed_eval_df = remove_hashtag_links(preprocessed_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Define your study_df, model_name, stream, and other necessary variables\n",
    "    stream = False  # Change to True if you want streaming\n",
    "\n",
    "    # Run the process_completions coroutine\n",
    "    generated_texts = await process_completions(preprocessed_eval_df, model_name, stream, batch_size=1000)\n",
    "    return generated_texts\n",
    "\n",
    "# Run the async main function\n",
    "generated_texts = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated = pd.DataFrame(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(eval_df_generated))\n",
    "print(eval_df_generated['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_df_generated['generated_text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated['EventType'] = eval_df_generated['generated_text']\\\n",
    "    .str\\\n",
    "    .lower()\\\n",
    "    .str\\\n",
    "    .contains(\"yes\")\\\n",
    "    .astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated['MatchID'] = eval_df_generated['text_idx'].apply(lambda x : x[0])\n",
    "eval_df_generated['PeriodID'] = eval_df_generated['text_idx'].apply(lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = eval_df.drop_duplicates(\"ID\")[['ID', \"MatchID\", \"PeriodID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    eval_df_generated,\n",
    "    x,\n",
    "    on = ['MatchID', \"PeriodID\"]\n",
    "    \n",
    ")[[\"ID\", \"EventType\"]]\\\n",
    "    .set_index(\"ID\")\\\n",
    "    .to_csv(\"predictions_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_generated['EventType'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"generated_summary_llm_2.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['text_idx']= df['text_idx'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'] = df['generated_text'].str.lower().str.contains(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60)\n",
      "After analyzing the tweets, I have the following summary:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"summary\": \"The minute saw a series of intense matches between Germany and Argentina, with both teams showing significant aggression and skill. Argentina's goalkeeper was praised for their good game, while Germany's fans expressed frustration with their team's performance. At the end of the minute, the score was still 0-0, but fans were already anticipating a goal.\",\n",
      "    \"goal\": \"no\",\n",
      "    \"cards\": \"yes\",\n",
      "    \"kickoff\": \"no\",\n",
      "    \"halftime\": \"yes\",\n",
      "    \"fulltime\": \"no\"\n",
      "}\n",
      "```\n",
      "\n",
      "Here's the explanation for each event:\n",
      "\n",
      "1. **Goal**: There is no clear indication of a goal scored in that exact minute. Some tweets mention that Argentina \"nicked it\" or that Germany \"was so close\", but these are speculative and do not explicitly mention a goal. [Tweet that refers to the goal: None]\n",
      "\n",
      "2. **Cards**: There is a mention of \"foul trouble\" for Germany, which implies that there were some yellow cards given out, but this does not explicitly state that there were any cards in that exact minute. However, the presence of this tweet suggests that there might have been some carding events in the match. [Tweet that refers to the card: \"Foul trouble for Germany\"]\n",
      "\n",
      "3. **Kickoff**: There is no indication that the match just started in that exact minute. The tweets suggest that the game has been ongoing for some time, with the teams already showing signs of fatigue and frustration. [Tweet that refers to the kickoff: None]\n",
      "\n",
      "4. **Halftime**: The tweet \"Half-time Argentina 0-0 Germany\" explicitly states that the halftime whistle has been blown, ending the first half. [Tweet that refers to the halftime: \"Half-time Argentina 0-0 Germany\"]\n",
      "\n",
      "5. **Fulltime**: There is no indication that the game has ended in that exact minute. The tweets suggest that the first half has ended, but the game is still ongoing. [Tweet that refers to the fulltime: None]\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(df.query(\"label == 0 and prediction == 1\")))\n",
    "print(df.query(\"label == 0 and prediction == 1\")['text_idx'].iloc[idx])\n",
    "print(df.query(\"label == 0 and prediction == 1\")['generated_text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 13)\n",
      "Welcome back Pepe! \n",
      "Also   \n",
      "Let's go  !!!!\n",
      "good luck for \n",
      "Let's go  !\n",
      "  -  \n",
      "LET'S GO  AND !!!\n",
      "Switching over to  \n",
      " &  lets goooooooo!\n",
      "do not disappoint me \n",
      "Kick off :  v \n",
      " &  all fucking day\n",
      "I'm With   & \n",
      "Do or die!   \n",
      "I'm prediction -2 -1\n",
      " and -3 -1\n",
      "Its do or die lets go! \n",
      " vs  \n",
      " vs  \n",
      "nice and sunny for  - \n",
      "ima need  to beat  today!!\n",
      "One  goal and we can relax\n",
      "THE BAE HES SO BEAUTIFYL  \n",
      "Games underway\n",
      " vs \n",
      " vs   \n",
      "\"0\" kick off  0-0  Live TVONE\"\n",
      "If only  had a World Class Striker..!!\n",
      "For   back in the first 11.\n",
      "\n",
      "Come on  !! Min 2-0!! Let's goo!!\n",
      "lmao no one on the left side \n",
      "eder is awful why is he there \n",
      "Watching  V  ..Hoping for nothing :(\n",
      "We can do this, I know we can \n",
      "Which channel is the  vs  match on?\n",
      "\n",
      "I really want  n  to win today\n",
      "hope  qualified for the next round ! \n",
      "I may be the only person watching  \n",
      "hoy voy con    y  \n",
      "Game on ! Btw   |  \n",
      "Let's go  we need the win, 3-0 \n",
      "I hope  wins and I hope  wins\n"
     ]
    }
   ],
   "source": [
    "print(df.query(\"label == 0 and prediction == 1\")['text_idx'].iloc[idx])\n",
    "print(df.query(\"label == 0 and prediction == 1\")['text'].iloc[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
