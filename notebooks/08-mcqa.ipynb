{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from src.utils import train_test_split\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from src.preprocessing import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, LoggingHandler\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/real_labels_1.csv\", index_col= 0 )\n",
    "train_df_2 = pd.read_csv(\"data/real_labels_2.csv\", index_col=0)\n",
    "train_df_3 = pd.read_csv(\"data/real_labels_3.csv\", index_col=0)\n",
    "\n",
    "train_df = pd.concat([train_df, train_df_2, train_df_3])\n",
    "train_df = train_df.drop_duplicates(\"originalTweet\")\n",
    "\n",
    "train_df['label'] = train_df['event'].map({\n",
    "    \"no\": 0,\n",
    "    \"yes\": 1,\n",
    "    \"penalty\" : 1,\n",
    "    \"goal\" : 1\n",
    "})\n",
    "\n",
    "train_df = train_df.sample(frac = 1., replace=False)\\\n",
    "    .dropna()\n",
    "\n",
    "val_df = train_df.iloc[20_000:]\n",
    "train_df = train_df.iloc[0:1_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train, test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "model =  AutoModel.from_pretrained(\"vinai/bertweet-base\", cache_dir = '/Data')\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", cache_dir = '/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    train_df['originalTweet'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=64\n",
    ")\n",
    "\n",
    "vectors = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vectors.last_hidden_state.numpy().reshape(1_000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "X = pca.fit_transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "y = train_df['label']\n",
    "\n",
    "# Define logistic regression model with class weights\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kfold = StratifiedKFold(n_splits=100, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "\n",
    "df = pd.concat(train.values())\n",
    "\n",
    "y = df['EventType']\n",
    "\n",
    "\n",
    "for train_idx, test_idx in kfold.split(df, y):\n",
    "    # Train-test split for the current fold\n",
    "    X_train, X_test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    print(len(X_test))\n",
    "    \n",
    "    tokens = tokenizer(\n",
    "        X_test['Tweet'].tolist(),\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    vectors = model(**tokens)\\\n",
    "        .last_hidden_state\\\n",
    "        .numpy()\\\n",
    "        .reshape(len(X_test), -1)\n",
    "\n",
    "    test_v = pca.fit_transform(vectors)\n",
    "        \n",
    "    X_test['proba_event'] = clf.predict_proba(test_v)[:, 1]\n",
    "    \n",
    "    # Apply threshold for classification\n",
    "    X_test['is_event'] = (X_test['proba_event'] > 0.5).astype(int)\n",
    "    \n",
    "    # Aggregate predictions by MatchID and PeriodID\n",
    "    pred_df = X_test.groupby(['MatchID', 'PeriodID'], as_index=False).agg({\n",
    "        'is_event': np.max,\n",
    "    })\n",
    "\n",
    "    # Merge back to original test_df for evaluation\n",
    "    merged_df = pd.merge(\n",
    "        X_test,\n",
    "        pred_df,\n",
    "        on=['MatchID', 'PeriodID']\n",
    "    )\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(merged_df['EventType'], merged_df['is_event_y'])\n",
    "    fold_accuracies.append(acc)\n",
    "    print(f\"Fold Accuracy: {acc}\")\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Mean Cross-Validation Accuracy: {mean_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45897839662505413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_702876/729903821.py:8: FutureWarning: The provided callable <function max at 0x7fe14c01d120> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  pred_df = df.groupby(['MatchID', \"PeriodID\"], as_index=False).agg({\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_i = vectorizer.transform(df['Tweet'])\n",
    "df['proba_event'] = clf.predict_proba(X_i)[:,1]\n",
    "\n",
    "# thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "# for t in thresholds:\n",
    "df['is_event'] = (df['proba_event'] >0.99).astype(int)\n",
    "\n",
    "pred_df = df.groupby(['MatchID', \"PeriodID\"], as_index=False).agg({\n",
    "    \"is_event\": np.max,\n",
    "})\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    df,\n",
    "    pred_df,\n",
    "    on=['MatchID', 'PeriodID']\n",
    ")\n",
    "acc = accuracy_score(merged_df['EventType'], merged_df['is_event_y'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventType\n",
       "1    471041\n",
       "0    399610\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['EventType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_event_y\n",
       "0    732510\n",
       "1    138141\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['is_event_y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
