{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from src.utils import train_test_split\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from src.preprocessing import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, LoggingHandler\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import fasttext\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_texts(x, min_size = 10):\n",
    "    size = x.apply(lambda x: len(x.split(\" \")))\\\n",
    "        .sort_values()\n",
    "    \n",
    "    x = x.reindex_like(size)\n",
    "    mask = size < min_size\n",
    "    # mask = x.str.lower().str.contains(\"goal\")\n",
    "\n",
    "    return \"\\n\".join(x[mask])\n",
    "    # return x[mask].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, test_data = train_test_split()\n",
    "\n",
    "def get_samples(indices, frac = 1):\n",
    "    all_df = []\n",
    "    for id in indices:\n",
    "        temp_df = train_data[id]\n",
    "        # for i in range(1,5+1):  \n",
    "        #     temp_df[f\"Tweet\"] = temp_df['Tweet'].shift(i) + temp_df['Tweet']\n",
    "        \n",
    "        all_df.append(temp_df.dropna().sample(frac=frac))\n",
    "\n",
    "        \n",
    "    return pd.concat(all_df).groupby([\"MatchID\", \"PeriodID\"]).agg({\n",
    "        \"Tweet\":    get_first_texts,\n",
    "        \"EventType\": np.mean,\n",
    "        \"ID\": len\n",
    "    })\n",
    "\n",
    "# train_df = get_samples(train_indices)\n",
    "# val_df = get_samples(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   8         #FRA is going to beat Ecuador good today(: And...\n",
       "    14        Let's go Honduras, with a little help from Fra...\n",
       "    29        The more goals France scores today the less go...\n",
       "    34        #ECU must match or better #SUI performance aga...\n",
       "    36        Robbie Fowler predicting a Honduras win? #Worl...\n",
       "                                    ...                        \n",
       "19  155519                                What a game 🙆⚽️🙌 #MEX\n",
       "    155480    Brazil vs Mexico in the knockout rounds? I'm r...\n",
       "    155478    Y'all already too worried about us going again...\n",
       "    155494    Holland is going to be a tough one but si se p...\n",
       "    155548    Don't know what happened to my other tweet but...\n",
       "Name: Tweet, Length: 1472980, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/1472980 [00:00<09:42, 2528.47it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "predict processes one line at a time (remove '\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlid.176.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mload_model(model_path)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlid.176.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mload_model(model_path)\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/fasttext/FastText.py:232\u001b[0m, in \u001b[0;36m_FastText.predict\u001b[0;34m(self, text, k, threshold, on_unicode_error)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_labels, all_probs\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mpredict(text, k, threshold, on_unicode_error)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m predictions:\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/fasttext/FastText.py:220\u001b[0m, in \u001b[0;36m_FastText.predict.<locals>.check\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(entry):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict processes one line at a time (remove \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    221\u001b[0m     entry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entry\n",
      "\u001b[0;31mValueError\u001b[0m: predict processes one line at a time (remove '\\n')"
     ]
    }
   ],
   "source": [
    "model_path = \"lid.176.bin\"\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "df['Tweet'].progress_apply(lambda x: model.predict(x, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/real_labels_1.csv\", index_col= 0 )\n",
    "train_df_2 = pd.read_csv(\"data/real_labels_2.csv\", index_col=0)\n",
    "\n",
    "train_df = pd.concat([train_df, train_df_2])\n",
    "train_df['label'] = train_df['event'].map({\n",
    "    \"no\": 0,\n",
    "    \"yes\": 1,\n",
    "    \"penalty\" : 1,\n",
    "    \"goal\" : 1\n",
    "})\n",
    "val_df = train_df.iloc[7_000:]\n",
    "train_df = train_df.iloc[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>reason</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th>originalTweet</th>\n",
       "      <th>EventType</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>Full time has been reached in both games in Gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>#NED 2 #CHI 0 - #ESP 3 #AUS 0. Fulltime in bot...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet is a general statement of support fo...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>rootin for the underdogs here go #ALG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not indicate a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>Okay now Algeria might lose..</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not describe a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>163</td>\n",
       "      <td>You did good #ALG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not represent a specific footba...</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>Algeria get your shit together and make us pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not describe a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>Damn, this Algerian goalie is in beast mode! #...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not describe a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>#ALG is plahing like beasts.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not represent a football event.</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>Oktoberfest is months away Germany please stop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not describe a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>162</td>\n",
       "      <td>Algeria are a fucking sick team, they've shock...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>yes</td>\n",
       "      <td>1st half end</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>spain in 1-0 lead Watch #Aus #Spain WC Match O...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    event                                             reason  MatchID  \\\n",
       "0     yes  Full time has been reached in both games in Gr...        2   \n",
       "1      no  The tweet is a general statement of support fo...        4   \n",
       "2      no  The tweet does not indicate a specific footbal...        4   \n",
       "3      no  The tweet does not describe a specific footbal...        4   \n",
       "4      no  The tweet does not represent a specific footba...        4   \n",
       "..    ...                                                ...      ...   \n",
       "995    no  The tweet does not describe a specific footbal...        4   \n",
       "996    no  The tweet does not describe a specific footbal...        4   \n",
       "997    no     The tweet does not represent a football event.        4   \n",
       "998    no  The tweet does not describe a specific footbal...        4   \n",
       "999   yes                                       1st half end        2   \n",
       "\n",
       "     PeriodID                                      originalTweet  EventType  \\\n",
       "0         121  #NED 2 #CHI 0 - #ESP 3 #AUS 0. Fulltime in bot...          1   \n",
       "1           9              rootin for the underdogs here go #ALG          0   \n",
       "2          74                      Okay now Algeria might lose..          0   \n",
       "3         163                                  You did good #ALG          1   \n",
       "4         144  Algeria get your shit together and make us pro...          1   \n",
       "..        ...                                                ...        ...   \n",
       "995       106  Damn, this Algerian goalie is in beast mode! #...          0   \n",
       "996        53                       #ALG is plahing like beasts.          0   \n",
       "997        60  Oktoberfest is months away Germany please stop...          1   \n",
       "998       162  Algeria are a fucking sick team, they've shock...          1   \n",
       "999        61  spain in 1-0 lead Watch #Aus #Spain WC Match O...          1   \n",
       "\n",
       "     label  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "995      0  \n",
       "996      0  \n",
       "997      0  \n",
       "998      0  \n",
       "999      1  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithExtraFeature(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, extra_feature_size=1):\n",
    "        super(BertWithExtraFeature, self).__init__()\n",
    "        # Load the pre-trained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name, cache_dir = '/Data')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.extra_feature_size = extra_feature_size\n",
    "        \n",
    "        # Fully connected layer to combine BERT output and extra feature\n",
    "        self.fc = torch.nn.Linear(self.hidden_size + self.extra_feature_size, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, extra_feature):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (batch_size, seq_len) with token IDs.\n",
    "            attention_mask: Tensor of shape (batch_size, seq_len) for masking attention.\n",
    "            extra_feature: Tensor of shape (batch_size, 1) with the additional feature.\n",
    "\n",
    "        Returns:\n",
    "            Logits for binary classification.\n",
    "        \"\"\"\n",
    "        # Get BERT output (pooled output)\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.pooler_output  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Concatenate the pooled output with the extra feature\n",
    "        combined_input = torch.cat((pooled_output, extra_feature), dim=1)  # Shape: (batch_size, hidden_size + extra_feature_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        logits = self.fc(combined_input)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "def evaluate_model(val_dataloader, model, device : str = 'cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type = 'cuda'):\n",
    "            for i,batch in tqdm(enumerate(val_dataloader), total = len(val_dataloader)):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                # count = batch['count'].to(device).unsqueeze(dim = -1)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                # if i % 100 == 0: \n",
    "                #     acc = accuracy_score(all_labels, all_preds)\n",
    "                #     f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "                #     clear_output()\n",
    "                #     print(f\"Validation Accuracy : {acc}\\n\")\n",
    "                #     print(f\"Validation F1 : {f1}\\n\")\n",
    "                #     conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "                #     print(conf_matrix)\n",
    "\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    clear_output()\n",
    "    print(f\"Validation Accuracy : {acc}\\n\")\n",
    "    print(f\"Validation F1 : {f1}\\n\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "    return torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event</th>\n",
       "      <th>reason</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th>originalTweet</th>\n",
       "      <th>EventType</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>The tweet suggests that Germany scored a goal,...</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>Took Germany awhile to score</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not indicate a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>Algeria are outplaying Germany at the moment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>The tweet is a general statement of support fo...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>big game for Algeria - hoping they can give Ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not describe a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>Germany has outplayed them this entire game, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>This tweet does not represent a specific footb...</td>\n",
       "      <td>4</td>\n",
       "      <td>161</td>\n",
       "      <td>please let this game play forever until Algeri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>6995</td>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not describe a specific footbal...</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>Germany looks like they wanna win 3rd place again</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>6996</td>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not represent a football event,...</td>\n",
       "      <td>4</td>\n",
       "      <td>102</td>\n",
       "      <td>Fuck off Algeria you cunts I got Germany in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>6997</td>\n",
       "      <td>yes</td>\n",
       "      <td>Kick-off of the Round of 16 match between Germ...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>In the Round of 16: teams that have won their ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>6998</td>\n",
       "      <td>no</td>\n",
       "      <td>The tweet does not represent a specific footba...</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>GOD BE WITH ALGERIA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>6999</td>\n",
       "      <td>yes</td>\n",
       "      <td>The tweet mentions a goal scored by Ozil, indi...</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>At least Ozil scored. Now it's time for German...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 event                                             reason  \\\n",
       "0              0   yes  The tweet suggests that Germany scored a goal,...   \n",
       "1              1    no  The tweet does not indicate a specific footbal...   \n",
       "2              2    no  The tweet is a general statement of support fo...   \n",
       "3              3    no  The tweet does not describe a specific footbal...   \n",
       "4              4    no  This tweet does not represent a specific footb...   \n",
       "...          ...   ...                                                ...   \n",
       "6995        6995    no  The tweet does not describe a specific footbal...   \n",
       "6996        6996    no  The tweet does not represent a football event,...   \n",
       "6997        6997   yes  Kick-off of the Round of 16 match between Germ...   \n",
       "6998        6998    no  The tweet does not represent a specific footba...   \n",
       "6999        6999   yes  The tweet mentions a goal scored by Ozil, indi...   \n",
       "\n",
       "      MatchID  PeriodID                                      originalTweet  \\\n",
       "0           4       132                       Took Germany awhile to score   \n",
       "1           4        26       Algeria are outplaying Germany at the moment   \n",
       "2           4        13  big game for Algeria - hoping they can give Ge...   \n",
       "3           4       125  Germany has outplayed them this entire game, t...   \n",
       "4           4       161  please let this game play forever until Algeri...   \n",
       "...       ...       ...                                                ...   \n",
       "6995        4       117  Germany looks like they wanna win 3rd place again   \n",
       "6996        4       102  Fuck off Algeria you cunts I got Germany in th...   \n",
       "6997        4        12  In the Round of 16: teams that have won their ...   \n",
       "6998        4       117                                GOD BE WITH ALGERIA   \n",
       "6999        4       160  At least Ozil scored. Now it's time for German...   \n",
       "\n",
       "      EventType  label  \n",
       "0             1      1  \n",
       "1             1      0  \n",
       "2             0      0  \n",
       "3             0      0  \n",
       "4             1      0  \n",
       "...         ...    ...  \n",
       "6995          0      0  \n",
       "6996          1      0  \n",
       "6997          0      1  \n",
       "6998          0      0  \n",
       "6999          1      1  \n",
       "\n",
       "[7000 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/pedro.silva/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 130/130 [00:08<00:00, 16.01it/s]\n",
      "100%|██████████| 390/390 [00:16<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35931742958845936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:08<00:00, 16.02it/s]\n",
      "100%|██████████| 390/390 [00:17<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35931742958845936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:08<00:00, 15.96it/s]\n",
      " 41%|████▏     | 161/390 [00:03<00:05, 40.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m         preds \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmax()  \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m.8\u001b[39m\n\u001b[1;32m     81\u001b[0m         y_true_val\u001b[38;5;241m.\u001b[39mextend([row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(texts))\n\u001b[0;32m---> 82\u001b[0m         y_pred_val\u001b[38;5;241m.\u001b[39mextend([\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(texts))\n\u001b[1;32m     84\u001b[0m acc_val \u001b[38;5;241m=\u001b[39m accuracy_score(y_true_val, y_pred_val)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = BertWithExtraFeature()\n",
    "model.to(device)\n",
    "\n",
    "labels = train_df[\"EventType\"].tolist()\n",
    "class_weights = compute_class_weights(labels).to(device)\n",
    "\n",
    "# Define weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir = '/Data')\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for idx, row in tqdm(train_df.iterrows(), total = len(train_df)):\n",
    "        texts = row['Tweet']\n",
    "        max_probs = []\n",
    "\n",
    "        batches =  [texts[i:i + 8] for i in range(0, len(texts), 8)]\n",
    "        for batch in batches:\n",
    "\n",
    "            tokens = tokenizer(\n",
    "                batch, \n",
    "                max_length=64,\n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            label = torch.tensor(row['EventType']).to(device)\n",
    "\n",
    "            input_ids = tokens[\"input_ids\"].to(device)\n",
    "            attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "            count = torch.ones(len(batch)).to(device).unsqueeze(dim = -1) * row['ID']\n",
    "\n",
    "            with torch.autocast( device_type = 'cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, extra_feature = count)\n",
    "                p = torch.softmax(outputs, dim = 1)\n",
    "\n",
    "                max_prob = p[:, 1].max()\n",
    "\n",
    "                max_probs.append(p[:, 1].max())\n",
    "\n",
    "        # Combine probabilities into a single tensor\n",
    "        max_prob = torch.stack(max_probs).max()\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = loss_fn(max_prob.unsqueeze(-1), label.unsqueeze(-1))\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    y_true_val =[]\n",
    "    y_pred_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(val_df.iterrows(), total = len(val_df)):\n",
    "            texts = row['originalTweet']\n",
    "\n",
    "            batch = tokenizer(\n",
    "                texts, \n",
    "                max_length=64,\n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            label = torch.tensor(row['EventType']).to(device)\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            # count = torch.ones(len(texts)).to(device).unsqueeze(dim = -1) * row['ID']\n",
    "\n",
    "            with torch.autocast( device_type = 'cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            p = torch.softmax(outputs.logiys, dim = -1)\n",
    "\n",
    "            y_true_val.extend([row['EventType']])\n",
    "            y_pred_val.extend([p.item()])\n",
    "\n",
    "    acc_val = accuracy_score(y_true_val, y_pred_val)\n",
    "    print(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2903, 0.7100],\n",
       "        [0.2776, 0.7227],\n",
       "        [0.2878, 0.7124],\n",
       "        [0.2698, 0.7305],\n",
       "        [0.2517, 0.7485],\n",
       "        [0.2959, 0.7041],\n",
       "        [0.2727, 0.7271],\n",
       "        [0.2705, 0.7295]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(outputs, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.42963792459873085\n",
      "\n",
      "Validation F1 : 0.0\n",
      "\n",
      "[[1151    0]\n",
      " [1528    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 28/110 [00:02<00:06, 11.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     66\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     68\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     70\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# K Fold CV\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir = '/Data')\n",
    "\n",
    "device = 'cuda'\n",
    "final_results = []\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    train_df[\"originalTweet\"].tolist(), \n",
    "    # train_df['ID'].tolist(),\n",
    "    train_df[\"EventType\"].tolist(), \n",
    "    tokenizer\n",
    ")\n",
    "val_dataset = TextDataset(\n",
    "    val_df[\"originalTweet\"].tolist(), \n",
    "    # val_df['ID'].tolist(),\n",
    "    val_df[\"EventType\"].tolist(), \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir = '/Data', num_labels = 2)\n",
    "model.to(device)\n",
    "\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "labels = train_df[\"EventType\"].tolist()\n",
    "class_weights = compute_class_weights(labels).to(device)\n",
    "\n",
    "# Define weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.autocast( device_type = 'cuda'):\n",
    "            # outputs = model(input_ids=input_ids, attention_mask=attention_mask, extra_feature = count)\n",
    "            # loss = loss_fn(outputs, labels.squeeze() )\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels = labels)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"---------- Epoch {epoch} ------------\")\n",
    "    print(f\"Training Loss : {epoch_loss}\\n\")\n",
    "    print(f\"Training Accuracy : {acc}\\n\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "    preds, labels = evaluate_model(val_dataloader, model)\n",
    "\n",
    "    acc = f1_score(labels, preds)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = deepcopy(model)\n",
    "\n",
    "\n",
    "# Combine results for this fold\n",
    "# validation_results = pd.DataFrame({\n",
    "#     \"MatchID\": validation_data[\"MatchID\"].values,\n",
    "#     \"true_values\": labels,\n",
    "#     \"predictions\": preds,\n",
    "# })\n",
    "\n",
    "# final_results.append(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5945, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(outputs.to(torch.float32), labels.to(torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.6152,  -3.4277],\n",
       "        [ -1.3516,  -2.8164],\n",
       "        [ -1.0127,  -1.6367],\n",
       "        [-15.9062, -44.3125],\n",
       "        [ -1.2471,  -2.8613],\n",
       "        [-11.7109, -32.1875],\n",
       "        [ -1.4023,  -3.0938],\n",
       "        [ -1.3418,  -3.2031],\n",
       "        [ -2.8711,  -7.0039],\n",
       "        [ -1.9004,  -4.3398],\n",
       "        [ -1.6602,  -3.9180],\n",
       "        [ -4.5391, -11.4531],\n",
       "        [ -0.8857,  -1.7021],\n",
       "        [-11.6875, -32.7188],\n",
       "        [ -1.9551,  -4.4180],\n",
       "        [ -0.7881,  -1.2773],\n",
       "        [-25.7656, -72.5000],\n",
       "        [-32.1562, -90.3750],\n",
       "        [ -0.8789,  -2.3047],\n",
       "        [-17.1406, -48.0938],\n",
       "        [-12.9766, -35.9688],\n",
       "        [ -5.4727, -14.6484],\n",
       "        [ -1.1064,  -2.1914],\n",
       "        [ -1.5566,  -3.9141],\n",
       "        [ -1.1553,  -2.1035],\n",
       "        [ -1.2471,  -2.4883],\n",
       "        [ -0.7754,  -1.1953],\n",
       "        [-11.2266, -30.7656],\n",
       "        [-13.0781, -36.0938],\n",
       "        [ -3.1875,  -8.4609],\n",
       "        [ -1.1475,  -2.2969],\n",
       "        [ -6.7539, -18.2344],\n",
       "        [-12.1250, -34.2188],\n",
       "        [ -1.7891,  -4.4609],\n",
       "        [ -0.9282,  -1.8525],\n",
       "        [-11.3125, -31.0312],\n",
       "        [-13.8594, -38.3750],\n",
       "        [ -0.8555,  -1.4238],\n",
       "        [ -1.1104,  -2.6641],\n",
       "        [ -1.2920,  -3.0195],\n",
       "        [ -1.0410,  -1.8906],\n",
       "        [ -1.7324,  -4.0625],\n",
       "        [ -4.8203, -12.5547],\n",
       "        [ -1.5146,  -3.3066],\n",
       "        [ -2.9883,  -8.1719],\n",
       "        [ -1.6553,  -4.3008],\n",
       "        [ -1.1758,  -2.9004],\n",
       "        [ -3.6348,  -9.1719],\n",
       "        [-13.9375, -39.2500],\n",
       "        [-14.3125, -39.8438],\n",
       "        [-16.7656, -46.5938],\n",
       "        [-30.1562, -85.1250],\n",
       "        [-17.6406, -49.5938],\n",
       "        [-14.5781, -40.7188],\n",
       "        [-27.4688, -76.9375],\n",
       "        [ -1.5195,  -3.6191],\n",
       "        [ -1.3350,  -2.8535],\n",
       "        [ -0.5889,  -1.3037],\n",
       "        [ -1.4922,  -3.6152],\n",
       "        [ -1.3203,  -2.5625],\n",
       "        [ -1.8887,  -4.5977],\n",
       "        [ -1.3232,  -2.6445],\n",
       "        [ -1.0322,  -2.1992],\n",
       "        [-16.4844, -45.7812]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [256, 1] doesn't match the broadcast shape [256, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/loss.py:734\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:3244\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [256, 1] doesn't match the broadcast shape [256, 2]"
     ]
    }
   ],
   "source": [
    "loss_fn(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1088],\n",
       "        [-0.0605],\n",
       "        [-0.0681],\n",
       "        [-0.0406],\n",
       "        [-0.1536],\n",
       "        [-0.1455],\n",
       "        [-0.0849],\n",
       "        [-0.1566],\n",
       "        [-0.0871],\n",
       "        [-0.0835],\n",
       "        [-0.0636],\n",
       "        [-0.1096],\n",
       "        [-0.2092],\n",
       "        [-0.0197],\n",
       "        [-0.0301],\n",
       "        [-0.0684],\n",
       "        [-0.1477],\n",
       "        [-0.2673],\n",
       "        [-0.1089],\n",
       "        [-0.0915],\n",
       "        [-0.0843],\n",
       "        [-0.0635],\n",
       "        [-0.1345],\n",
       "        [-0.0330],\n",
       "        [-0.0521],\n",
       "        [-0.1393],\n",
       "        [-0.1578],\n",
       "        [-0.0182],\n",
       "        [ 0.0490],\n",
       "        [-0.0577],\n",
       "        [-0.0142],\n",
       "        [-0.0567],\n",
       "        [-0.1060],\n",
       "        [-0.0703],\n",
       "        [-0.1915],\n",
       "        [-0.0572],\n",
       "        [-0.1100],\n",
       "        [-0.2637],\n",
       "        [-0.0435],\n",
       "        [-0.0605],\n",
       "        [-0.0484],\n",
       "        [-0.0782],\n",
       "        [-0.1560],\n",
       "        [-0.0133],\n",
       "        [-0.0787],\n",
       "        [-0.0480],\n",
       "        [-0.1337],\n",
       "        [-0.0585],\n",
       "        [ 0.0135],\n",
       "        [-0.0414],\n",
       "        [-0.0280],\n",
       "        [-0.1719],\n",
       "        [-0.0699],\n",
       "        [-0.0728],\n",
       "        [-0.2644],\n",
       "        [-0.0791],\n",
       "        [-0.1698],\n",
       "        [-0.3916],\n",
       "        [-0.1609],\n",
       "        [-0.0381],\n",
       "        [-0.1267],\n",
       "        [ 0.0065],\n",
       "        [-0.0784],\n",
       "        [-0.1190],\n",
       "        [-0.0795],\n",
       "        [-0.1501],\n",
       "        [-0.0867],\n",
       "        [-0.0668],\n",
       "        [-0.1255],\n",
       "        [-0.0671],\n",
       "        [-0.1377],\n",
       "        [-0.1367],\n",
       "        [-0.1500],\n",
       "        [-0.3975],\n",
       "        [-0.1432],\n",
       "        [-0.3135],\n",
       "        [-0.1835],\n",
       "        [-0.0131],\n",
       "        [-0.0936],\n",
       "        [-0.2208],\n",
       "        [-0.0321],\n",
       "        [-0.0289],\n",
       "        [-0.1324],\n",
       "        [-0.1226],\n",
       "        [ 0.0145],\n",
       "        [-0.1295],\n",
       "        [-0.1868],\n",
       "        [-0.0814],\n",
       "        [-0.0158],\n",
       "        [-0.0807],\n",
       "        [-0.3179],\n",
       "        [-0.1456],\n",
       "        [-0.0298],\n",
       "        [ 0.0324],\n",
       "        [-0.0454],\n",
       "        [-0.0200],\n",
       "        [-0.2056],\n",
       "        [-0.1448],\n",
       "        [-0.0967],\n",
       "        [-0.2449],\n",
       "        [-0.0996],\n",
       "        [-0.1045],\n",
       "        [-0.1071],\n",
       "        [ 0.0007],\n",
       "        [-0.0656],\n",
       "        [-0.1256],\n",
       "        [-0.1655],\n",
       "        [-0.0193],\n",
       "        [-0.1792],\n",
       "        [-0.0884],\n",
       "        [-0.0525],\n",
       "        [-0.0075],\n",
       "        [-0.0641],\n",
       "        [-0.0294],\n",
       "        [-0.1619],\n",
       "        [-0.0587],\n",
       "        [-0.0514],\n",
       "        [-0.1289],\n",
       "        [-0.2079],\n",
       "        [-0.1277],\n",
       "        [-0.0878],\n",
       "        [-0.2515],\n",
       "        [-0.1293],\n",
       "        [-0.2183],\n",
       "        [-0.0615],\n",
       "        [-0.1219],\n",
       "        [-0.0426],\n",
       "        [-0.0672],\n",
       "        [-0.0637],\n",
       "        [-0.1256],\n",
       "        [-0.1986],\n",
       "        [-0.2974],\n",
       "        [-0.0357],\n",
       "        [-0.0789],\n",
       "        [-0.0581],\n",
       "        [-0.1833],\n",
       "        [-0.0197],\n",
       "        [-0.1361],\n",
       "        [-0.1587],\n",
       "        [-0.0303],\n",
       "        [-0.0845],\n",
       "        [-0.0681],\n",
       "        [-0.0260],\n",
       "        [-0.1624],\n",
       "        [-0.0701],\n",
       "        [-0.0961],\n",
       "        [-0.0828],\n",
       "        [-0.1619],\n",
       "        [-0.1946],\n",
       "        [-0.0942],\n",
       "        [-0.0837],\n",
       "        [-0.0610],\n",
       "        [-0.0214],\n",
       "        [-0.1484],\n",
       "        [-0.1769],\n",
       "        [-0.3530],\n",
       "        [-0.0766],\n",
       "        [-0.0564],\n",
       "        [-0.0941],\n",
       "        [-0.1311],\n",
       "        [-0.0354],\n",
       "        [-0.3120],\n",
       "        [-0.0248],\n",
       "        [-0.0751],\n",
       "        [-0.0611],\n",
       "        [-0.0869],\n",
       "        [-0.0905],\n",
       "        [-0.0197],\n",
       "        [-0.1536],\n",
       "        [-0.0800],\n",
       "        [-0.1407],\n",
       "        [-0.0309],\n",
       "        [-0.0341],\n",
       "        [-0.0335],\n",
       "        [-0.1072],\n",
       "        [-0.0995],\n",
       "        [-0.1063],\n",
       "        [-0.1210],\n",
       "        [-0.2256],\n",
       "        [-0.1490],\n",
       "        [-0.0723],\n",
       "        [-0.0679],\n",
       "        [-0.0429],\n",
       "        [-0.2295],\n",
       "        [-0.1073],\n",
       "        [-0.0429],\n",
       "        [-0.1349],\n",
       "        [-0.0447],\n",
       "        [-0.0325],\n",
       "        [-0.0412],\n",
       "        [-0.1449],\n",
       "        [-0.1439],\n",
       "        [-0.0074],\n",
       "        [ 0.0145],\n",
       "        [-0.0548],\n",
       "        [ 0.0402],\n",
       "        [-0.0287],\n",
       "        [-0.1379],\n",
       "        [-0.0632],\n",
       "        [-0.1742],\n",
       "        [ 0.0067],\n",
       "        [-0.0631],\n",
       "        [-0.0975],\n",
       "        [-0.0984],\n",
       "        [-0.0403],\n",
       "        [-0.0724],\n",
       "        [-0.0263],\n",
       "        [-0.1124],\n",
       "        [-0.0349],\n",
       "        [-0.0092],\n",
       "        [-0.2021],\n",
       "        [-0.1052],\n",
       "        [-0.1271],\n",
       "        [-0.1016],\n",
       "        [-0.0813],\n",
       "        [-0.0430],\n",
       "        [-0.0844],\n",
       "        [-0.0670],\n",
       "        [-0.0975],\n",
       "        [-0.1267],\n",
       "        [-0.2517],\n",
       "        [-0.1493],\n",
       "        [-0.0535],\n",
       "        [-0.1821],\n",
       "        [-0.1364],\n",
       "        [-0.0170],\n",
       "        [-0.0454],\n",
       "        [-0.0848],\n",
       "        [-0.2408],\n",
       "        [-0.0413],\n",
       "        [-0.2085],\n",
       "        [-0.0436],\n",
       "        [ 0.0197],\n",
       "        [-0.1003],\n",
       "        [-0.0330],\n",
       "        [-0.2954],\n",
       "        [-0.0983],\n",
       "        [-0.0450],\n",
       "        [-0.0925],\n",
       "        [-0.1182],\n",
       "        [-0.0458],\n",
       "        [-0.0972],\n",
       "        [-0.1693],\n",
       "        [-0.0128],\n",
       "        [-0.0884],\n",
       "        [-0.0574],\n",
       "        [-0.3408],\n",
       "        [-0.0833],\n",
       "        [-0.1322],\n",
       "        [-0.1092],\n",
       "        [-0.1240],\n",
       "        [-0.0478],\n",
       "        [-0.1204],\n",
       "        [-0.1554],\n",
       "        [-0.1313],\n",
       "        [-0.0616]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.6384615384615384\n",
      "\n",
      "Validation F1 : 0.7486631016042781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds, labels = evaluate_model(val_dataloader, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [14,18,19]\n",
    "\n",
    "test_df = get_samples(test_indices)\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    test_df[\"Tweet\"].tolist(), test_df[\"EventType\"].tolist(), tokenizer\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.7124\n",
      "\n",
      "Validation F1 : 0.8080619327282434\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "preds, labels = evaluate_model(test_dataloader, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
